{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_keys = ['memory', 'localisa', 'rnn', 'memoriz', 'state space', 'long.*term' ]\n",
    "pomdp_keys  = ['pomdp', '(partially)? *observ', '(?<!collaborative) filteri', 'belief']\n",
    "rl_keys = ['mdp', 'markov', 'bellman', '\\\\bimitation', 'policy', '\\\\bthompson', '\\\\bvalue f', 'critic', 'reward', 'state', '\\\\baction', '\\\\bactor', 'exploration', 'backup',\n",
    "                   '\\\\brl\\\\b', 'reinforce', 'agent', 'bandit', 'mcts', 'monte carlo tree search', 'meta.?.?.?learnin', 'q.?.?.?.?learn',\n",
    "           'environm'] \n",
    "rl_keys += pomdp_keys\n",
    "var_seq_keys = ['variationa', 'sequent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_any(title, keywords):\n",
    "    for k in keywords:\n",
    "        if re.search(k, title, re.I):\n",
    "            return k\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2018\n",
      "Generative Temporal Models with Spatial Memory for Partially Observed Environments\n",
      "Regret Minimization for Partially Observable Deep Reinforcement Learning\n",
      "Deep Variational Reinforcement Learning for POMDPs\n",
      "Learning to Act in Decentralized Partially Observable MDPs\n",
      "Learning Registered Point Processes from Idiosyncratic Observations\n",
      "\n",
      "\n",
      "2019\n",
      "Provably efficient RL with Rich Observations via Latent State Decoding\n",
      "Convolutional Poisson Gamma Belief Network\n",
      "Active Learning for Decision-Making from Imbalanced Observational Data\n",
      "A Block Coordinate Descent Proximal Method for Simultaneous Filtering and Parameter Estimation\n",
      "Switching Linear Dynamics for Variational Bayes Filtering\n",
      "Imitating Latent Policies from Observation\n",
      "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations\n",
      "Provably Efficient Imitation Learning from Observation Alone\n",
      "Submodular Observation Selection and Information Gathering for Quadratic Models\n",
      "Neural Separation of Observed and Unobserved Distributions\n"
     ]
    }
   ],
   "source": [
    "# for y in [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]:\n",
    "for y in [2018, 2019]:\n",
    "    nips_link = f'https://nips.cc/Conferences/{y}/Schedule?type=Poster'\n",
    "    icml_link = f'https://icml.cc/Conferences/{y}/Schedule?type=Poster'\n",
    "\n",
    "    link = icml_link\n",
    "    \n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    papers = [o.text for o in soup.select(\".maincardBody\")]\n",
    "    authors = [o.text for o in soup.select(\".maincardFooter\")]\n",
    "    \n",
    "    pomdp_papers = [p for p in papers if contains_any(p, pomdp_keys)]\n",
    "    print('\\n\\n' +  str(y).upper())\n",
    "    for p in pomdp_papers:\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv-Sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_sanity_link = 'http://www.arxiv-sanity.com/search?q=pomdp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ipaulo/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/ipaulo/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(arxiv_sanity_link)\n",
    "for i in range(50):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    sleep(0.1)\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Inverse POMDP: Inferring What You Think from What You Do',\n",
       " 'Hindsight is Only 50/50: Unsuitability of MDP based Approximate POMDP\\n  Solvers for Multi-resolution Information Gathering',\n",
       " 'DESPOT: Online POMDP Planning with Regularization',\n",
       " 'Supervisor Synthesis of POMDP based on Automata Learning',\n",
       " 'A Programming Language With a POMDP Inside',\n",
       " 'A Function Approximation Approach to Estimation of Policy Gradient for\\n  POMDP with Structured Policies',\n",
       " 'The Actor Search Tree Critic (ASTC) for Off-Policy POMDP Learning in\\n  Medical Decision Making',\n",
       " 'Learning States Representations in POMDP',\n",
       " 'Efficient Hierarchical Robot Motion Planning Under Uncertainty and\\n  Hybrid Dynamics',\n",
       " \"'Indifference' methods for managing agent rewards\",\n",
       " 'Incorrigibility in the CIRL Framework',\n",
       " 'Variational Inference for Data-Efficient Model Learning in POMDPs',\n",
       " 'Planning to Give Information in Partially Observed Domains with a\\n  Learned Weighted Entropy Model',\n",
       " 'Sequential Multi-Class Labeling in Crowdsourcing',\n",
       " 'Maximizing Expected Impact in an Agent Reputation Network -- Technical\\n  Report',\n",
       " 'Deep Hierarchical Reinforcement Learning Algorithm in Partially\\n  Observable Markov Decision Processes',\n",
       " 'Bounded Policy Synthesis for POMDPs with Safe-Reachability Objectives',\n",
       " 'Intent-aware Multi-agent Reinforcement Learning',\n",
       " 'Human-in-the-Loop Synthesis for Partially Observable Markov Decision\\n  Processes',\n",
       " 'Novel Sensor Scheduling Scheme for Intruder Tracking in Energy Efficient\\n  Sensor Networks',\n",
       " 'Counterfactual equivalence for POMDPs, and underlying deterministic\\n  environments',\n",
       " 'Planning with Trust for Human-Robot Collaboration',\n",
       " 'A Decision-theoretic Approach to Detection-based Target Search with a\\n  UAV',\n",
       " 'Sensor Synthesis for POMDPs with Reachability Objectives',\n",
       " 'Octopus: A Framework for Cost-Quality-Time Optimization in Crowdsourcing',\n",
       " 'Motion Planning under Partial Observability using Game-Based Abstraction',\n",
       " 'Belief Tree Search for Active Object Recognition',\n",
       " 'On Automating the Doctrine of Double Effect',\n",
       " 'Tracking as Online Decision-Making: Learning a Policy from Streaming\\n  Videos with Reinforcement Learning',\n",
       " 'Personalizing a Dialogue System with Transfer Reinforcement Learning',\n",
       " 'Learning to Represent Haptic Feedback for Partially-Observable Tasks',\n",
       " 'Experimental results : Reinforcement Learning of POMDPs using Spectral\\n  Methods',\n",
       " 'A Refinement-Based Architecture for Knowledge Representation and\\n  Reasoning in Robotics',\n",
       " 'The Value of Inferring the Internal State of Traffic Participants for\\n  Autonomous Freeway Driving',\n",
       " 'Optimizing Expectation with Guarantees in POMDPs (Technical Report)',\n",
       " 'Cooperative Inverse Reinforcement Learning',\n",
       " 'Open Problem: Approximate Planning of POMDPs in the class of Memoryless\\n  Policies',\n",
       " 'A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning\\n  and Plan Caching',\n",
       " 'Reinforcement Learning of POMDPs using Spectral Methods',\n",
       " 'Stochastic Shortest Path with Energy Constraints in POMDPs',\n",
       " 'Optimal Recommendation to Users that React: Online Learning for a Class\\n  of POMDPs',\n",
       " 'Optimizing Gaze Direction in a Visual Navigation Task',\n",
       " 'Optimal Radio Frequency Energy Harvesting with Limited Energy Arrival\\n  Knowledge',\n",
       " 'Two Timescale Convergent Q-learning for Sleep--Scheduling in Wireless\\n  Sensor Networks',\n",
       " 'Nonmyopic View Planning for Active Object Detection',\n",
       " 'PEGASUS: A Policy Search Method for Large MDPs and POMDPs',\n",
       " 'Policy Improvement for POMDPs Using Normalized Importance Sampling',\n",
       " 'Monte Carlo Bayesian Reinforcement Learning',\n",
       " 'Apprenticeship Learning for Model Parameters of Partially Observable\\n  Environments']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.text for o in soup.select('.ts a')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
